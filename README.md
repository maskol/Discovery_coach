# Discovery Coach Documentation

**Discovery Coach** is an intelligent SAFe (Scaled Agile Framework) coaching application that helps Product Managers, Epic Owners, and Product Leaders create well-defined Epic Hypothesis Statements, Features, and PI Objectives through Socratic discovery coaching.

## Documentation Structure

This documentation is split into focused modules for easier navigation and maintenance:

### Core Documentation
- **[01-OVERVIEW.md](01-OVERVIEW.md)** - System overview, capabilities, and use cases
- **[02-ARCHITECTURE.md](02-ARCHITECTURE.md)** - System architecture and design patterns
- **[03-FEATURES.md](03-FEATURES.md)** - Detailed feature descriptions and workflows
- **[04-API-REFERENCE.md](04-API-REFERENCE.md)** - Complete API endpoint documentation
- **[05-USER-GUIDE.md](05-USER-GUIDE.md)** - How to use Discovery Coach effectively

### Technical Documentation
- **[BACKEND_SETUP.md](BACKEND_SETUP.md)** - Backend configuration and setup
- **[FRONTEND.md](FRONTEND.md)** - Frontend implementation details
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Deployment and operations guide
- **[CHANGELOG.md](CHANGELOG.md)** - Recent updates and version history

## Quick Start

### Prerequisites
- Python 3.13+
- **Either** OpenAI API key **OR** Ollama installed (for local LLMs)
- Virtual environment: `venv/`
- ‚ö†Ô∏è **IMPORTANT**: Disable VPN for optimal OpenAI API connectivity (not needed for Ollama)

### Launch Discovery Coach
```bash
cd Discovery_coach
./start.sh
```

The server will:
1. Activate virtual environment
2. Initialize RAG vector database (7 knowledge base documents)
3. Start FastAPI server on `http://localhost:8050`
4. Automatically open GUI in default browser

### Alternative Manual Start
```bash
# Activate virtual environment
source venv/bin/activate

# Start backend server
uvicorn app:app --host 0.0.0.0 --port 8050 --reload

# Access GUI at http://localhost:8050
```

### Performance Expectations
- Summary generation: 4-6 seconds
- Regular chat with RAG: 2-4 seconds
- RAG retrieval: <1 second (6 documents, ~5KB context)
- No hanging or timeouts

## Key Features at a Glance

‚úÖ **Web-Based GUI** - Modern chat interface with real-time responses  
‚úÖ **Socratic Discovery** - Guided questioning (WHO/WHAT/WHY/IMPACT)  
‚úÖ **RAG-Enhanced** - Knowledge base integration via Chroma vector DB  
‚úÖ **Session Management** - Save/load/delete conversation sessions  
‚úÖ **Auto-Detection** - Automatically extracts and stores Epics/Features  
‚úÖ **Draft Buttons** - One-click Epic and Feature generation  
‚úÖ **Summary View** - Show discovery progress at any time  
‚úÖ **Model Selection** - Choose between GPT-4o, GPT-4o-mini, GPT-o1, etc.  
‚úÖ **Local LLM Support** - Use Ollama for completely private, offline processing  
‚úÖ **Temperature Control** - Adjust response creativity (0-2)  
‚úÖ **Conversation Continuity** - Follow-up questions after summaries/outlines  
‚úÖ **MVP & Full Scope** - Epic template includes MVP and forecasted rollout  

## Technology Stack

| Component | Technology |
|-----------|-----------|
| Frontend | HTML5, CSS3, Vanilla JavaScript |
| Backend | FastAPI 0.115.0 + Uvicorn 0.38.0 (Python 3.13) |
| LLM Framework | LangChain 1.1.2 + langchain-openai 1.1.0 + langchain-ollama 0.2.0+ |
| Language Model | OpenAI GPT-4o-mini **OR** Ollama (llama3.2, mistral, etc.) |
| Vector Database | ChromaDB 1.0+ with text-embedding-3-small or Ollama embeddings |
| Session Storage | Server-side JSON files in `Session_storage/` |
| Knowledge Base | 7 documents in `knowledge_base/` |
| Port | 8050 (configurable) |
| Timeout Protection | Frontend: 30-180s, LLM: 60s |

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Web Browser   ‚îÇ  frontend/index.html + script.js + styles.css
‚îÇ   (Frontend)    ‚îÇ  Provider Selection: OpenAI or Ollama
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ HTTP/JSON :8050
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FastAPI       ‚îÇ  backend/app.py (REST API endpoints)
‚îÇ   (Backend)     ‚îÇ  backend/discovery_coach.py (RAG core)
‚îÇ                 ‚îÇ  backend/ollama_config.py (Ollama integration)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº                   ‚ñº              ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM Provider ‚îÇ  ‚îÇ ChromaDB     ‚îÇ  ‚îÇ Prompt      ‚îÇ  ‚îÇ Sessions     ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ (RAG)        ‚îÇ  ‚îÇ Templates   ‚îÇ  ‚îÇ Storage      ‚îÇ
‚îÇ ‚îÇ OpenAI   ‚îÇ ‚îÇ  ‚îÇ rag_db/      ‚îÇ  ‚îÇ data/       ‚îÇ  ‚îÇ data/        ‚îÇ
‚îÇ ‚îÇ GPT-4o   ‚îÇ ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ Embeddings:  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ      OR      ‚îÇ  ‚îÇ - OpenAI     ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ - Ollama     ‚îÇ
‚îÇ ‚îÇ Ollama   ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ ‚îÇ llama3.2 ‚îÇ ‚îÇ
‚îÇ ‚îÇ (Local)  ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Project Structure

```
Discovery_coach/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ .env                         # OpenAI API key (not in git)
‚îú‚îÄ‚îÄ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ backend/                     # Python backend code
‚îÇ   ‚îú‚îÄ‚îÄ app.py                  # FastAPI server + REST endpoints
‚îÇ   ‚îú‚îÄ‚îÄ discovery_coach.py      # RAG initialization & core logic
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ frontend/                    # Web UI
‚îÇ   ‚îú‚îÄ‚îÄ index.html              # Main GUI
‚îÇ   ‚îú‚îÄ‚îÄ script.js               # Frontend logic
‚îÇ   ‚îî‚îÄ‚îÄ styles.css              # Styling
‚îÇ
‚îú‚îÄ‚îÄ scripts/                     # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ start.sh                # Start server (detailed)
‚îÇ   ‚îú‚îÄ‚îÄ stop.sh                 # Stop server
‚îÇ   ‚îú‚îÄ‚îÄ status.sh               # Check status
‚îÇ   ‚îî‚îÄ‚îÄ reset_knowledge.sh      # Rebuild RAG database
‚îÇ
‚îú‚îÄ‚îÄ data/                        # Data and storage
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base/         # RAG source documents (7 files)
‚îÇ   ‚îú‚îÄ‚îÄ prompt_help/            # System prompts & questionnaires
‚îÇ   ‚îî‚îÄ‚îÄ Session_storage/        # Saved conversation sessions
‚îÇ
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Documentation index
‚îÇ   ‚îú‚îÄ‚îÄ BACKEND_SETUP.md        # Backend setup guide
‚îÇ   ‚îú‚îÄ‚îÄ 03-FEATURES.md          # Feature documentation
‚îÇ   ‚îú‚îÄ‚îÄ 04-API-REFERENCE.md     # API reference
‚îÇ   ‚îî‚îÄ‚îÄ CHANGELOG.md            # Version history
‚îÇ
‚îú‚îÄ‚îÄ rag_db/                      # ChromaDB vector database (auto-generated)
‚îÇ   ‚îî‚îÄ‚îÄ chroma.sqlite3          # ~164KB, 7 documents indexed
‚îÇ
‚îú‚îÄ‚îÄ venv/                        # Python virtual environment
‚îÇ
‚îî‚îÄ‚îÄ start.sh, stop.sh, status.sh # Root-level convenience scripts
```
         ‚îÇ HTTP/JSON (Port 8050)
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FastAPI Server ‚îÇ  app.py (REST API)
‚îÇ   (Backend)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LangChain RAG  ‚îÇ  ‚îÇ Session Storage  ‚îÇ
‚îÇ  discovery_coach‚îÇ  ‚îÇ (JSON files)     ‚îÇ
‚îÇ  .py            ‚îÇ  ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ OpenAI GPT   ‚îÇ  ‚îÇ Chroma   ‚îÇ  ‚îÇ Knowledge    ‚îÇ
‚îÇ (LLM)        ‚îÇ  ‚îÇ Vector DB‚îÇ  ‚îÇ Base (.txt)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Local LLM Support with Ollama üè†

Discovery Coach now supports **Ollama** for completely private, offline LLM processing! This means you can:
- üîí Keep sensitive data on your machine - nothing sent to external APIs
- üåê Work completely offline once models are downloaded  
- üí∞ Avoid API costs - Ollama is free and open source
- ‚ö° Choose models that match your hardware capabilities

### Quick Ollama Setup

1. **Install Ollama**: Visit [ollama.ai](https://ollama.ai) or use Homebrew:
   ```bash
   brew install ollama
   ```

2. **Pull Models**:
   ```bash
   ollama pull llama3.2:latest         # Chat model (recommended)
   ollama pull nomic-embed-text:latest # Embeddings (required for RAG)
   ```

3. **Start Ollama**:
   ```bash
   ollama serve
   ```

4. **Select in UI**: In Discovery Coach, choose **üè† Local (Ollama)** in Model Settings

üìñ **Full Setup Guide**: See [OLLAMA_SETUP.md](OLLAMA_SETUP.md) for detailed instructions, model recommendations, and troubleshooting.

## Recent Updates (December 2025)

### Ollama Integration (Dec 19)
- ‚úÖ Local LLM support via Ollama for private, offline processing
- ‚úÖ Dynamic provider selection: External (OpenAI) or Local (Ollama)
- ‚úÖ Ollama embeddings support for RAG vector database
- ‚úÖ Connection status monitoring and model discovery
- ‚úÖ Comprehensive setup documentation and troubleshooting guide

### Architecture Improvements (Dec 8)
- ‚úÖ Refactored `discovery_coach.py` - removed ~450 lines of unused CLI code (76% reduction)
- ‚úÖ Clean separation: API module functions only, no CLI interface
- ‚úÖ Improved maintainability with focused, single-purpose module
- ‚úÖ Updated virtual environment path from `../.venv` to `venv/`

### Knowledge Base Enhancements (Dec 8)
- ‚úÖ Epic template now includes 19 sections (was 15)
- ‚úÖ Added Section 11: FORECASTED COSTS (MVP + Full Implementation)
- ‚úÖ Added Section 14: SOLUTION ANALYSIS (customer, program, sales impact)
- ‚úÖ Added Section 15: DEVELOPMENT STRATEGY (sourcing, phasing, dependencies)
- ‚úÖ Comprehensive validation checklist in template usage notes
- ‚úÖ Enhanced example with complete cost, solution, and strategy sections
- ‚úÖ Feature selection workflow - system now asks which feature to draft
- ‚úÖ Updated .gitignore to exclude *.sqlite3, *.bin, chroma.sqlite3

### Session Management Enhancements
- ‚úÖ Server-side session storage in `Session_storage/` folder
- ‚úÖ Session list view with metadata (date, size)
- ‚úÖ Multi-select session deletion with confirmation
- ‚úÖ Session load restores full chat history

### Conversation Flow Improvements
- ‚úÖ Automatic Epic/Feature detection and storage
- ‚úÖ Draft Epic and Draft Feature buttons
- ‚úÖ Follow-up questions after Summary/Outline
- ‚úÖ Continuous conversation context
- ‚úÖ Feature selection prompt when multiple features proposed

### UI/UX Enhancements
- ‚úÖ Model selection dropdown (5 models)
- ‚úÖ Temperature slider (0-2 range)
- ‚úÖ Loading indicators for all async operations
- ‚úÖ Grouped action buttons (Epic/Feature)
- ‚úÖ Left-aligned user messages for better readability

## Support & Troubleshooting

### Common Issues

**Server won't start (port in use):**
```bash
./stop.sh  # Kills processes on port 8050
./start.sh # Restarts server
```

**Knowledge base not updating:**
```bash
./reset_knowledge.sh  # Removes rag_db folder
# Restart server to rebuild
```

**Check server status:**
```bash
./status.sh  # Shows server, DB, and session info
```

### Getting Help

1. Check the detailed documentation in this folder
2. Review API docs at `http://localhost:8050/docs` (when server running)
3. Examine server logs for error messages
4. Verify `.env` file contains valid `OPENAI_API_KEY`

## Contributing

When adding features or fixing bugs:

1. Update relevant documentation files
2. Test with `./start.sh` and `./stop.sh` scripts
3. Verify session save/load functionality
4. Check Epic/Feature auto-detection works
5. Update CHANGELOG.md with changes

## License

[Include license information here]
