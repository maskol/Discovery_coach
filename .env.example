# Discovery Coach Environment Configuration

# ============================================================================
# OpenAI Configuration (for External LLM Provider)
# ============================================================================
# Required when using External (OpenAI) LLMs
OPENAI_API_KEY=your_openai_api_key_here

# ============================================================================
# Ollama Configuration (for Local LLM Provider)
# ============================================================================
# Base URL for Ollama service
# Default: http://localhost:11434
# Change this if running Ollama on a different machine or port
OLLAMA_BASE_URL=http://localhost:11434

# Default chat model to use with Ollama
# Must be a model you've pulled with: ollama pull <model>
# Recommended options:
#   - llama3.2:latest (8B params, balanced performance)
#   - llama3.2:3b (3B params, faster, lower quality)
#   - mistral:latest (7B params, high quality)
OLLAMA_CHAT_MODEL=llama3.2:latest

# Embedding model for RAG/vector search
# This model is used for converting text to embeddings
# Must be pulled with: ollama pull nomic-embed-text:latest
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest

# ============================================================================
# LangChain Configuration
# ============================================================================
# Disable telemetry to avoid connection errors
LANGCHAIN_TRACING_V2=false
LANGCHAIN_CALLBACKS_BACKGROUND=false

# ============================================================================
# Instructions
# ============================================================================
# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. For OpenAI usage:
#    - Add your OpenAI API key
#
# 3. For Ollama usage:
#    - Install Ollama: https://ollama.ai
#    - Pull models: ollama pull llama3.2:latest
#    - Pull embeddings: ollama pull nomic-embed-text:latest
#    - Start service: ollama serve
#
# 4. Restart the Discovery Coach backend to load new settings
